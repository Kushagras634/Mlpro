{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8db5a2e5",
   "metadata": {},
   "source": [
    "# **Project: Prediction based on the most efficient classification model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66bacd",
   "metadata": {},
   "source": [
    "### Loading necessary libraries needed for this project\n",
    "\n",
    "We are using a lot of predefined functions which are imported from various libraries. The main libraries that we are using are:\n",
    "- nltk\n",
    "- skit-learn\n",
    "- NumPy\n",
    "- matplotlib\n",
    "- string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae836b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f111ecb",
   "metadata": {},
   "source": [
    "**Newsgroups Data and List of Stopwords(in English)**\n",
    "\n",
    "We are also initializing four lists:\n",
    "- xp and yp are initialized for plotting a bar graph of accuracy vs feature extraction\n",
    "- xpc and ypc are initialized for plotting a bar graph of accuracy vs classifier model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d60df416",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Newsgroups : ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Stopwords : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "xpc=[]\n",
    "ypc=[]\n",
    "xp=[]\n",
    "yp=[]\n",
    "print('Total Newsgroups :',fetch_20newsgroups(subset='all').target_names)\n",
    "print()\n",
    "print(\"Stopwords :\",stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e654a9e",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "Dataset=20 Newsgroups\n",
    "\n",
    "We are splitting the dataset into three parts: training, developing, and testing sets. We will be using training and developing datasets to modify and refine our vocabulary to achieve maximum accuracy for the different classification models.\n",
    "\n",
    "- X_train and Y_train are training datasets on which we are going to train our model.\n",
    "- X_dev and Y_dev are developement datasets on which are going to develope our feature extraction models.\n",
    "- X_test and Y_test are testing datasets on which we are testing our classifier models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ce42ec",
   "metadata": {},
   "source": [
    "*The alternative way of data collection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaca2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 11314\n",
      "Development dataset: 4519\n",
      "Test dataset: 3013\n"
     ]
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train')\n",
    "data_test = fetch_20newsgroups(subset='test')\n",
    "X_train,Y_train=data_train.data,data_train.target\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(data_test.data, data_test.target, test_size=0.40, random_state=24)\n",
    "print('Training dataset:', len(X_train))\n",
    "print('Development dataset:', len(X_dev))\n",
    "print('Test dataset:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a92fb2",
   "metadata": {},
   "source": [
    "*The way we are using for this project(By splitting the data from the complete 20 newsgroup)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf8d384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 14134\n",
      "Development dataset: 2827\n",
      "Test dataset: 1885\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 24\n",
    "\n",
    "dataset_news=fetch_20newsgroups(subset='all')\n",
    "X_train, X_intermediate, Y_train, Y_intermediate = train_test_split(dataset_news.data,dataset_news.target, test_size=0.25, random_state=RANDOM_STATE)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_intermediate, Y_intermediate, test_size=0.40, random_state=RANDOM_STATE)\n",
    "print('Training dataset:', len(X_train))\n",
    "print('Development dataset:', len(X_dev))\n",
    "print('Test dataset:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f578b83",
   "metadata": {},
   "source": [
    "### Data Wrangling or Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ea678",
   "metadata": {},
   "source": [
    "**Feature Extraction 1: Without preprocessing the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d407e",
   "metadata": {},
   "source": [
    "Applying simple CountVectorizer() function without using any kind of data wrangling techniques.\n",
    "\n",
    "This is to check whether the dataset which has stopwords, irrelevant data, or is having some missing data, can be efficient for the classification model or other refined datasets will have better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37103007",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction(wthout any preprocessing): vocabulary size is 143335 in 14134 documents\n",
      "NB classifier accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "feature_extraction_1 = CountVectorizer(analyzer= 'word')\n",
    "Vocabulary_training = feature_extraction_1.fit_transform(X_train)\n",
    "print('Feature extraction(wthout any preprocessing): vocabulary size is {} in {} documents'.format(Vocabulary_training.shape[1], Vocabulary_training.shape[0]))\n",
    "\n",
    "Vocabulary_developing= feature_extraction_1.transform(X_dev)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocabulary_training, Y_train)\n",
    "pred = classifier_NB.predict(Vocabulary_developing)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_dev, pred),4)))\n",
    "xp.append(\"FE1\")\n",
    "yp.append(round(metrics.accuracy_score(Y_dev, pred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c188933",
   "metadata": {},
   "source": [
    "Function Stem_token() for stemming of texts or tokens,using Porter's Algorithm using the function PorterStemmer() \n",
    "from from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450fc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem_token(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98932c",
   "metadata": {},
   "source": [
    "Function Stem_stopword_tokenize() for stemming of stopwords , by using the above function Stem_token() ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e8a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=[]\n",
    "def Stem_stopword_tokenize(li):\n",
    "    for w in li:\n",
    "        ws=Stem_token(w)\n",
    "        if len(ws)>1:\n",
    "            for i in range(len(ws)):\n",
    "                stop_words.append(ws[i])\n",
    "        else:\n",
    "            stop_words.append(ws[0])\n",
    "    return stop_words\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26b0ee",
   "metadata": {},
   "source": [
    "**Feature Extraction 2: By tokenization, stemming and removal of stopwords and punctuations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1563f73b",
   "metadata": {},
   "source": [
    "We are using CountVectorizer() function with parameters like: \n",
    "- tokenizer:which will tokenize the dataset.\n",
    "- stop_words:For removing stop_words and punctuations and extra 2 elements :``,becau\n",
    "\n",
    "Under these conditions, we are checking whether the datasets don't have any stopwords and punctuations has a better model accuracy or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54811864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction 2: vocabulary size is 176885 in 14134 documents\n",
      "NB classifier accuracy: 0.8939\n"
     ]
    }
   ],
   "source": [
    "stop_words=Stem_stopword_tokenize(stopwords.words('english'))+ list(string.punctuation)+['``', 'becau']\n",
    "Feature_extraction_2= CountVectorizer(analyzer= 'word', tokenizer=Stem_token,\n",
    "                               stop_words=stop_words)\n",
    "Vocabulary_training = Feature_extraction_2.fit_transform(X_train)\n",
    "print('Feature extraction 2: vocabulary size is {} in {} documents'.format(Vocabulary_training.shape[1], Vocabulary_training.shape[0]))\n",
    "\n",
    "Vocabulary_developing = Feature_extraction_2.transform(X_dev)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocabulary_training, Y_train)\n",
    "pred = classifier_NB.predict(Vocabulary_developing)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_dev, pred),4)))\n",
    "xp.append(\"FE2\")\n",
    "yp.append(round(metrics.accuracy_score(Y_dev, pred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68280b71",
   "metadata": {},
   "source": [
    "**Feature Extraction 3:By tokenization, stemming, removing accents, removal of stopwords and punctuations and performing other character normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720946e",
   "metadata": {},
   "source": [
    "We are using CountVectorizer() function with parameters like: \n",
    "- tokenizer:which will tokenize the dataset.\n",
    "- stop_words:For removing stop_words and punctuations and extra 2 elements :``,becau\n",
    "- strip_accents:For removing accents and performing other character normalization\n",
    "- lowercase:Converting all to lowercase before tokenizing\n",
    "- ngram_range:(1,2)means unigrams and bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a473b89f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction 3: vocabulary size is 1441567 in 14134 documents\n",
      "NB classifier accuracy: 0.9162\n"
     ]
    }
   ],
   "source": [
    "Feature_extraction_3 = CountVectorizer(analyzer= 'word', tokenizer=Stem_token,\n",
    "                                stop_words=Stem_stopword_tokenize(stopwords.words('english'))+ list(string.punctuation)+['``', 'becau'],\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2))\n",
    "Vocabulary_training= Feature_extraction_3.fit_transform(X_train)\n",
    "print('Feature extraction 3: vocabulary size is {} in {} documents'.format(Vocabulary_training.shape[1], Vocabulary_training.shape[0]))\n",
    "\n",
    "Vocabulary_developing = Feature_extraction_3.transform(X_dev)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocabulary_training, Y_train)\n",
    "pred = classifier_NB.predict(Vocabulary_developing)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_dev, pred),4)))\n",
    "xp.append(\"FE3\")\n",
    "yp.append(round(metrics.accuracy_score(Y_dev, pred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcdd256",
   "metadata": {},
   "source": [
    "**Feature Extraction 4:Same parameters as the Feature Extraction 3 but using a different function TfidfVectorizer() for tokenization of words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f91be",
   "metadata": {},
   "source": [
    "Count Vectorizer give number of frequency with respect to index of vocabulary where as tf-idf consider overall documents of weight of words.\n",
    "Hence, in this Feature extraction algorithm we are using TfidVectorizer instead of CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b19bb6f",
   "metadata": {},
   "source": [
    "We are using TfidVectorizer() function with parameters like: \n",
    "- tokenizer:which will tokenize the dataset.\n",
    "- stop_words:For removing stop_words and punctuations and extra 2 elements :``,becau\n",
    "- strip_accents:For removing accents and performing other character normalization\n",
    "- lowercase:Converting all to lowercase before tokenizing\n",
    "- ngram_range:(1,2)means unigrams and bigrams\n",
    "- min_df: Ignoring terms that have a document frequency strictly lower than the given threshold.\n",
    "- max_df: ignoring terms that have a document frequency strictly higher than the given threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1497bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction 4: vocabulary size is 86096 in 14134 documents\n",
      "NB classifier accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "Feature_extraction_4 = TfidfVectorizer(analyzer= 'word', tokenizer=Stem_token,\n",
    "                                stop_words=Stem_stopword_tokenize(stopwords.words('english'))+ list(string.punctuation)+['``', 'becau'],\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2),\n",
    "                                min_df=5, max_df= 0.75)\n",
    "Vocabulary_training = Feature_extraction_4.fit_transform(X_train)\n",
    "print('Feature extraction 4: vocabulary size is {} in {} documents'.format(Vocabulary_training.shape[1], Vocabulary_training.shape[0]))\n",
    "\n",
    "Vocabulary_developing = Feature_extraction_4.transform(X_dev)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocabulary_training, Y_train)\n",
    "pred = classifier_NB.predict(Vocabulary_developing)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_dev, pred),4)))\n",
    "xp.append(\"FE4\")\n",
    "yp.append(round(metrics.accuracy_score(Y_dev, pred),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578dfb79",
   "metadata": {},
   "source": [
    "**Finalized Feature Extraction Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec68ec6",
   "metadata": {},
   "source": [
    "From the graph given below, we know that feature extraction 4  has the best NB classifier accuracy, so we are going to use it for our classification models and prediction algorithm. \n",
    "\n",
    "Our finalized Feature extraction is Feature_extraction_4 as it has the maximum NB classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565ef1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMIklEQVR4nO3cX4xc91mH8eeL3aCipgSRBYodsCXcglU1AUyKVEWE8qd2UmEqepEUWhG1siJhVCQQMRfARW+oKlBBcbGsYEUVCF+Q0JriEhVBCVUp8roKSZ3ganFKsnWkbCj/UpAiJy8XO4FhMt6dtWd3dt88H2mlOef8dubVsffx8dmdTVUhSdr6vmHWA0iSpsOgS1ITBl2SmjDoktSEQZekJrbP6oWvv/762rVr16xeXpK2pLNnzz5XVXPjjs0s6Lt27WJ+fn5WLy9JW1KSf77cMW+5SFITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMze6eopFevXUf+fNYjzNRXfuv2dXlegy5dAYO0PkHS1fGWiyQ1YdAlqQmDLklNbMl76N6/9P6lpFfakkHX1fMfRf9RVD/ecpGkJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYmCnqS/UnOJ1lIcmTM8W9O8mdJ/iHJuSR3TX9USdJKVg16km3AUeAAsBe4M8nekWW/ADxeVTcCtwK/neSaKc8qSVrBJFfoNwMLVXWhql4ATgIHR9YUcG2SAK8DvgZcmuqkkqQVTRL0HcDTQ9uLg33D7gW+D7gIPAZ8sKpeGn2iJIeSzCeZX1pausKRJUnjTBL0jNlXI9vvAB4BvhO4Cbg3yetf8UlVx6tqX1Xtm5ubW+OokqSVTBL0ReCGoe2dLF+JD7sLeLCWLQBPAt87nRElSZOYJOhngD1Jdg++0XkHcGpkzVPAjwEk+XbgTcCFaQ4qSVrZ9tUWVNWlJIeBh4BtwImqOpfk7sHxY8CHgPuTPMbyLZp7quq5dZxbkjRi1aADVNVp4PTIvmNDjy8CPznd0SRJa+E7RSWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSExMFPcn+JOeTLCQ5cpk1tyZ5JMm5JH8z3TElSavZvtqCJNuAo8BPAIvAmSSnqurxoTXXAR8D9lfVU0m+bZ3mlSRdxiRX6DcDC1V1oapeAE4CB0fWvAd4sKqeAqiqZ6c7piRpNZMEfQfw9ND24mDfsDcC35Lks0nOJnnfuCdKcijJfJL5paWlK5tYkjTWJEHPmH01sr0d+EHgduAdwK8neeMrPqnqeFXtq6p9c3Nzax5WknR5q95DZ/mK/Iah7Z3AxTFrnquqrwNfT/IwcCPw5alMKUla1SRX6GeAPUl2J7kGuAM4NbLmk8AtSbYn+SbgrcAT0x1VkrSSVa/Qq+pSksPAQ8A24ERVnUty9+D4sap6IslfAI8CLwH3VdWX1nNwSdL/N8ktF6rqNHB6ZN+xke2PAB+Z3miSpLXwnaKS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqYmJgp5kf5LzSRaSHFlh3Q8leTHJu6c3oiRpEqsGPck24ChwANgL3Jlk72XWfRh4aNpDSpJWN8kV+s3AQlVdqKoXgJPAwTHrfhF4AHh2ivNJkiY0SdB3AE8PbS8O9v2vJDuAdwHHVnqiJIeSzCeZX1paWuuskqQVTBL0jNlXI9sfBe6pqhdXeqKqOl5V+6pq39zc3IQjSpImsX2CNYvADUPbO4GLI2v2ASeTAFwP3JbkUlV9YhpDSpJWN0nQzwB7kuwGvgrcAbxneEFV7X75cZL7gU8Zc0naWKsGvaouJTnM8k+vbANOVNW5JHcPjq9431yStDEmuUKnqk4Dp0f2jQ15Vf381Y8lSVor3ykqSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpqYKOhJ9ic5n2QhyZExx382yaODj88nuXH6o0qSVrJq0JNsA44CB4C9wJ1J9o4sexL4kap6C/Ah4Pi0B5UkrWySK/SbgYWqulBVLwAngYPDC6rq81X1r4PNLwA7pzumJGk1kwR9B/D00PbiYN/lvB/49LgDSQ4lmU8yv7S0NPmUkqRVTRL0jNlXYxcmP8py0O8Zd7yqjlfVvqraNzc3N/mUkqRVbZ9gzSJww9D2TuDi6KIkbwHuAw5U1b9MZzxJ0qQmuUI/A+xJsjvJNcAdwKnhBUm+C3gQeG9VfXn6Y0qSVrPqFXpVXUpyGHgI2AacqKpzSe4eHD8G/AbwrcDHkgBcqqp96ze2JGnUJLdcqKrTwOmRfceGHn8A+MB0R5MkrYXvFJWkJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTUwU9CT7k5xPspDkyJjjSfJ7g+OPJvmB6Y8qSVrJqkFPsg04ChwA9gJ3Jtk7suwAsGfwcQj4/SnPKUlaxSRX6DcDC1V1oapeAE4CB0fWHAQ+Xsu+AFyX5A1TnlWStILtE6zZATw9tL0IvHWCNTuAZ4YXJTnE8hU8wPNJzq9p2s3jeuC5Wb14PjyrV54qz+HV8fxdna18/r77cgcmCXrG7KsrWENVHQeOT/Cam1qS+araN+s5tjLP4dXx/F2drudvklsui8ANQ9s7gYtXsEaStI4mCfoZYE+S3UmuAe4ATo2sOQW8b/DTLj8M/HtVPTP6RJKk9bPqLZequpTkMPAQsA04UVXnktw9OH4MOA3cBiwA/wXctX4jbwpb/rbRJuA5vDqev6vT8vyl6hW3uiVJW5DvFJWkJgy6JDUxyY8tvuokeRF4bGjXTwO7gE8CTw7t/5Wq+sskJ4B3As9W1Zs3as7Nai3nDzgPfBz4DuAl4HhV/e6GDLpJrfH8fQ54GPhGlr+e/6SqfnNDBt3E1vo1PPicbcA88NWqeufGTDpdBn28/66qm4Z3JNkF/O1l/qDvB+5lOUxaw/kbvKP4l6vqi0muBc4m+UxVPb5h024+azl/Ad5eVc8neQ3wuSSfHrxj+9VsrV/DAB8EngBev76jrR9vuUxBVT0MfG3Wc2xFVfVMVX1x8Pg/Wf6C2jHbqbaOwa/beH6w+ZrBhz/psEZJdgK3A/fNepar4RX6eK9N8sjg8ZNV9a7B41uG9gP8TFX904ZOtjVc0fkbXEF9P/D3GzHkJram8ze4VXAW+B7gaFW92s8frP3v4EeBXwWu3bAJ14FBH+8V/10bWOm/a/o/az5/SV4HPAD8UlX9x3oOtwWs6fxV1YvATUmuA/40yZur6kvrPONmN/E5TPLy97/OJrl1A2ZbN95y0cwN7v0+APxRVT0463m2qqr6N+CzwP7ZTrLlvA34qSRfYfm3yb49yR/OdqQrY9A1U4Nv6v0B8ERV/c6s59lqkswNrsxJ8lrgx4F/nOlQW0xV/VpV7ayqXSz/apO/qqqfm/FYV8Sgr80tSR4Z+ng3QJI/Bv4OeFOSxSTvn+2Ym9a48/c24L0sXxW9vP+2Gc+5WY07f28A/jrJoyz/3qXPVNWnZjvmpjb2a7gL3/ovSU14hS5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ18T/GBimugwVZhgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(xp)\n",
    "y= np.array(yp)\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c45adde",
   "metadata": {},
   "source": [
    "### Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a5f1f",
   "metadata": {},
   "source": [
    "**Classifier Models: Multinomial Naive Bayes(MultinomialNB()) classifier, Logistic Regression(LogisticRegression()) and Stochastic Gradient Descent(SGDClassifier())**\n",
    "\n",
    "We are using three different machine learning techniques for the classification model and we will select the one, which will have the best model accuracy .\n",
    "These 3 Machine Learning Techniques are:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e3f3d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB classifier accuracy: 0.9066\n",
      "Logistic regression classifier accuracy: 0.9151\n",
      "Stochastic Gradient Descent classifier accuracy: 0.9188\n"
     ]
    }
   ],
   "source": [
    "Vocabulary_test = Feature_extraction_4.transform(X_test)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocabulary_training, Y_train)\n",
    "predict = classifier_NB.predict(Vocabulary_test)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, predict),4)))\n",
    "xpc.append(\"NB classifier\")\n",
    "ypc.append(round(metrics.accuracy_score(Y_test, predict),4))\n",
    "\n",
    "classifier_logistic_regression = LogisticRegression(penalty = 'l2', solver='sag', C=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_logistic_regression.fit(Vocabulary_training, Y_train)\n",
    "predict = classifier_logistic_regression.predict(Vocabulary_test)\n",
    "print(\"Logistic regression classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, predict),4)))\n",
    "xpc.append(\"LR classifier\")\n",
    "ypc.append(round(metrics.accuracy_score(Y_test, predict),4))\n",
    "\n",
    "classifier_SGD = SGDClassifier(tol=0.0001, penalty=\"l2\", alpha=0.0001, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_SGD.fit(Vocabulary_training, Y_train)\n",
    "predict = classifier_SGD.predict(Vocabulary_test)\n",
    "print(\"Stochastic Gradient Descent classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, predict),4)))\n",
    "xpc.append(\"SGD classifier\")\n",
    "ypc.append(round(metrics.accuracy_score(Y_test, predict),4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9190fc2",
   "metadata": {},
   "source": [
    "**Graph of Accuracy vs Classification models**\n",
    "\n",
    "   Accuracies are given as:\n",
    "- NB classifier accuracy: 0.9058\n",
    "- Logistic regression classifier accuracy: 0.9151\n",
    "- Stochastic Gradient Descent classifier accuracy: 0.9202\n",
    "\n",
    "From the graph we can see that the SGDclassifier or Stochastic Gradient Descent classifier has the best accuracy for predicting input corpus into one of the 20 newsgroups.However, multinomial NB has marginally lower accuracy but perfoms significantly much faster than the other classifiers on the new test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "247a10e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQI0lEQVR4nO3df7BcZX3H8ffHhBSRX9pcUAM01FIwFmUgRUvHotVSou3QWmYEtVYqZXAaf9BaZWpHqXQUxh84FjATFKm/ih2lFTWCrSPaKeIkSPgRNJjiD2KcklSrIioGvv3jnOCybO7dkL2E++T9mrnDnud59uz37nPz2XOe3T2kqpAkzX2P2tUFSJImw0CXpEYY6JLUCANdkhphoEtSI+bvqgdeuHBhLV68eFc9vCTNSddff/2Wqpoa1bfLAn3x4sWsWbNmVz28JM1JSb61vT6XXCSpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqRG77Juikh7ZFp/96V1dQrO+ed7zZ2W/BroeFobD7JmtcNDcMycD3XCYPYaDNHe5hi5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEaMFehJTkyyPsmGJGeP6N8vySeT3JhkXZLTJl+qJGk6MwZ6knnARcAyYAlwapIlQ8P+Eri1qp4GPAt4R5IFE65VkjSNcY7QjwU2VNXtVXUPcDlw0tCYAvZJEmBv4HvA1olWKkma1jiBvgi4Y2B7Y9826ELgycAm4Gbg1VV13/COkpyRZE2SNZs3b36IJUuSRhkn0DOirYa2fx9YCzwROAq4MMm+D7pT1cqqWlpVS6empnawVEnSdMYJ9I3AwQPbB9EdiQ86DbiiOhuAbwBHTKZESdI4xgn01cBhSQ7t3+g8BbhyaMy3gecAJDkQOBy4fZKFSpKmN3+mAVW1Ncly4GpgHnBpVa1LcmbfvwI4F7gsyc10SzSvr6ots1i3JGnIjIEOUFWrgFVDbSsGbm8CTphsaZKkHeE3RSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUiLECPcmJSdYn2ZDk7O2MeVaStUnWJfnCZMuUJM1k/kwDkswDLgJ+D9gIrE5yZVXdOjBmf+Bi4MSq+naSA2apXknSdoxzhH4ssKGqbq+qe4DLgZOGxrwIuKKqvg1QVXdOtkxJ0kzGCfRFwB0D2xv7tkG/Djw2yTVJrk/y0kkVKEkaz4xLLkBGtNWI/RwDPAd4NPClJNdV1W0P2FFyBnAGwCGHHLLj1UqStmucI/SNwMED2wcBm0aMuaqqflxVW4AvAk8b3lFVrayqpVW1dGpq6qHWLEkaYZxAXw0cluTQJAuAU4Arh8Z8AnhmkvlJ9gKeDnx1sqVKkqYz45JLVW1Nshy4GpgHXFpV65Kc2fevqKqvJrkKuAm4D3hvVd0ym4VLkh5onDV0qmoVsGqobcXQ9tuAt02uNEnSjvCbopLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqxFiBnuTEJOuTbEhy9jTjfjPJvUlOnlyJkqRxzBjoSeYBFwHLgCXAqUmWbGfc+cDVky5SkjSzcY7QjwU2VNXtVXUPcDlw0ohxrwQ+Dtw5wfokSWMaJ9AXAXcMbG/s2+6XZBHwx8CKyZUmSdoR4wR6RrTV0Pa7gNdX1b3T7ig5I8maJGs2b948ZomSpHHMH2PMRuDgge2DgE1DY5YClycBWAg8L8nWqvq3wUFVtRJYCbB06dLhFwVJ0k4YJ9BXA4clORT4DnAK8KLBAVV16LbbSS4DPjUc5pKk2TVjoFfV1iTL6T69Mg+4tKrWJTmz73fdXJIeAcY5QqeqVgGrhtpGBnlVvWzny5Ik7Si/KSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktSIsQI9yYlJ1ifZkOTsEf0vTnJT/3NtkqdNvlRJ0nRmDPQk84CLgGXAEuDUJEuGhn0DOL6qngqcC6ycdKGSpOmNc4R+LLChqm6vqnuAy4GTBgdU1bVV9f1+8zrgoMmWKUmayTiBvgi4Y2B7Y9+2PS8HPjOqI8kZSdYkWbN58+bxq5QkzWicQM+Itho5MHk2XaC/flR/Va2sqqVVtXRqamr8KiVJM5o/xpiNwMED2wcBm4YHJXkq8F5gWVX972TKkySNa5wj9NXAYUkOTbIAOAW4cnBAkkOAK4A/rarbJl+mJGkmMx6hV9XWJMuBq4F5wKVVtS7JmX3/CuCNwC8DFycB2FpVS2evbEnSsHGWXKiqVcCqobYVA7dPB06fbGmSpB3hN0UlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IixAj3JiUnWJ9mQ5OwR/Uny7r7/piRHT75USdJ0Zgz0JPOAi4BlwBLg1CRLhoYtAw7rf84A3jPhOiVJMxjnCP1YYENV3V5V9wCXAycNjTkJ+EB1rgP2T/KECdcqSZrG/DHGLALuGNjeCDx9jDGLgO8ODkpyBt0RPMBdSdbvULVz10Jgy64uYhw5f1dX8IjhnM0tc2a+YKfn7Fe21zFOoGdEWz2EMVTVSmDlGI/ZlCRrqmrprq5D43PO5hbnqzPOkstG4OCB7YOATQ9hjCRpFo0T6KuBw5IcmmQBcApw5dCYK4GX9p92eQbwg6r67vCOJEmzZ8Yll6rammQ5cDUwD7i0qtYlObPvXwGsAp4HbADuBk6bvZLnpN1umakBztnc4nwBqXrQUrckaQ7ym6KS1AgDXZIasVsFepJK8o6B7dcmOae/fU6S7yRZm+RrSd6TZOznJ8ldE6zzzCQv7W8f0dd0Q5InJbl2Uo8zV4x6bofm69Ykp+7sPneivt16vpK8Icm6/rIfa5M8vW+fn+QtSb7et69N8oaB+93bt61LcmOSv9rBf3OXJTl5Qr/DE5N8bGD7n/vf56wkb07y3Ek8zmwb53PoLfkZ8IIkb62qUV9CuKCq3t7/UX0ROB74/MNaIfe/0bzNHwGfqKo39dvHjbufJKF7n+S+CZb3SLJtvg4Drk/ysar6+cNdxO48X0l+C/gD4Oiq+lmShcCCvvsfgMcDR1bVT5PsA/z1wN1/UlVH9fs5APgIsB/wJh5mVbUJOLmv5fHAcVW13S/wTCfJ/KraOsn6xrVbHaEDW+neDT9rhnELgD2B7w93JDkwyb/2RxQ3JjluqH/vJJ9L8pUkNyc5qW9/TJJP9/e5JckL+/bz+iPMm5K8vW87pz97eB7wGuD0JJ/v++4aeKy/SbK6v+/f922Lk3w1ycXAV3jg9wOaVFVfp/t01WOH+5yvWfcEYEtV/QygqrZU1aYkewF/Abyyqn7a9/2oqs4ZtZOqupPuW+TL+xe2B0jyun5+bkxy3oj+N/bP7S1JVm7bR5JXDczX5X3b8QNnDDck2aefh1v63X0WOKDvf+bgmUCSY5J8Icn1Sa5Of4mTJNekOxv5AvDqnXg+d05V7TY/wF3AvsA36Y4EXguc0/edA3wHWEsX5B/Zzj4+Crymvz0P2G/bvvv/zgf27W8vpPsoZ4A/AS4Z2M9+wOOA9fzi00b7D9Ty2uHbQ49zAt2LU+hemD8F/A6wGLgPeMaufr4nOW8j2gafo6OB/3S+dsnc7N3/m7kNuBg4vm9/KnDDQ5jX7wMHDrUtA64F9uq3H9f/9zLg5MG2/vYHgT/sb28Cfmlovj4J/PZA/fP7ebilb7v/9uDjAHv0dUz17S+k+xg3wDXAxbt6Pna3I3Sq6ofAB4BXjei+oLpTwAOAxyQ5ZcSY36W/mmRV3VtVPxjqD/CWJDcB/0F3TZsDgZuB5yY5P8kz+/v9EPgp8N4kL6A7yhzXCf3PDXRHdkfQXe0S4FvVXSStdWelux7Ql+mCdBTnaxZV1V3AMXRH15uBjyZ52fC4JKf1R7x3JJnuLGTUZUSeC7y/qu7uH/N7I8Y8O8mXk9xMN+dP6dtvAj6c5CV0Z+gA/wW8M8mr6EJ+3OWRw4HfAP49yVrg7+i+Fb/NR8fcz6zZ7QK99y7g5cBjRnVWtw57Fd0R1I56MTAFHNO/OPwPsGdV3Ub3h38z8NYkb+z/kI4FPk639nrVDjxOgLdW1VH9z69V1fv6vh8/hLrnoguq6nC6I6UPJNnzIezD+dpJ/QvlNdW9b7Cc7uxmA3BIunVzqur9/fP7A7ozpQdJ8qvAvcCdw12MuDbUwP32pDs7OLmqjgQuoVsyBXg+3eW/j6F7n2V+VZ0HnA48GrguyRFj/qoB1g3M4ZFVdcJA/y6fx90y0PtX+H+hC/UH6dffjgP+e0T354BX9OPmJdl3qH8/4M6q+nmSZ9NfGS3JE4G7q+pDwNuBo5PsTbcEsIpu7fWoHfg1rgb+vN8HSRale2Npt1NVVwBrgD8b0e18zaIkh6d7U3qbo+jOOO4G3gdcuO2FNt3/W2HBg/cCSaaAFcCF1a9hDPgs3XO3Vz/2cUP928J7S//8blvvfhRwcFV9HngdsD+wd5InVdXNVXU+3d/NuIG+HphK90YwSfZI8pQZ7vOw2t0+5TLoHXRHE4PO6k/N9qA7Vbt4xP1eDaxM8nK6o4lXAF8a6P8w8Mkka+jWFr/Wtx8JvC3JfcDP+/vtA3yi/4MPM79Ze7+q+mySJwNf6t//uQt4SV9Ta/ZKsnFg+50jxrwZ+EiSS+qBnxJxvmbX3sA/JtmfbkljA7+4RPYbgHOBW5L8CPgJ8E/84sJ9j+6XLvbo7/tBRsxtVV2V5ChgTZJ76C418rcD/f+X5BK6s6lv0l1/CrozgQ8l2Y9uvi7ox57bv3jfC9wKfIbuzd1pVdU9/Zuj7+73OZ/ubH/dTPd9uPjVf0lqxG655CJJLTLQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiP+H6t/d3so5lsXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xc = np.array(xpc)\n",
    "yc= np.array(ypc)\n",
    "plt.bar(xc,yc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882c072",
   "metadata": {},
   "source": [
    "### Prediction algorithm based Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f50e7b",
   "metadata": {},
   "source": [
    "AS SGD classifier has the best accuracy we are going to use this for predicting input text(corpus) using the predictingNewsGroup() function which prints the following:\n",
    "\n",
    "Predicted newsgroup: filename\n",
    "\n",
    "For this I am using 5 sentences from the 20 newsgroups dataset to check its integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40f5f621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted newsgroup: alt.atheism\n",
      "Predicted newsgroup: sci.crypt\n",
      "Predicted newsgroup: rec.sport.hockey\n",
      "Predicted newsgroup: rec.autos\n",
      "Predicted newsgroup: talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "def predictingNewsGroup(text, classifier):\n",
    "    Vocabulary_test = Feature_extraction_4.transform([text])\n",
    "    targets = dataset_news.target_names\n",
    "    idx = classifier.predict(Vocabulary_test)\n",
    "    print(\"Predicted newsgroup: {}\".format(targets[int(idx)]))\n",
    "    return\n",
    "\n",
    "print()\n",
    "predictingNewsGroup(\"My argument is that the sole purpose of the death penalty is tok ill people\", classifier_SGD)\n",
    "predictingNewsGroup(\"RIPEM is available via anonymous FTP to citizens and permanent residents in the U.S\", classifier_SGD)\n",
    "predictingNewsGroup(\"Honorable mentions to Majestic Marty and Warren Rychel\", classifier_SGD)\n",
    "predictingNewsGroup(\"left turn signal won't stop automaticaly\", classifier_SGD)\n",
    "predictingNewsGroup(\"here is it difficult to obtain handguns legally for protection, registration figures are meaningless.\", classifier_SGD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
